0
지금부터 A609 팀의 발표를 시작하겠습니다.<C>

1
여러분은 지금 청각 장애인과의 대화를 간접적으로 체험해보았습니다.
한 공간에서 서로를 마주하고 있는 상황에서도 텍스트를 통해 의사소통하게 되면 대화에 있어 많은 시간이 소요되고, 서로의 눈을 바라보지 못하는 등의 많은 제약 사항이 생깁니다.
다시 한번 인사드리겠습니다.
안녕하십니까. A609팀 손톡의 발표를 맡은 김용우 입니다.<C>

2
손톡팀은 청각장애인과 비장애인의 대화의 벽<C> 을 허무는 것<C>에 초점을 두고 프로젝트를 기획하였습니다.

3
현재 이런 서비스를 제공하기 위한 노력이 없는 것은 아닙니다.
청각장애인은 예시에서와 같이 문자 소통을 하며 의사소통을 하는데, 빠른 실시간 소통의 한계가 있고<C>
슬레잇과 같은 수화 통역 서비스가 제공되고 있지만, 한국어를 지원하지 않으며<C>
높은 정확도를 위해 장갑을 활용한 통역 서비스도 있지만 추가로 장비를 챙겨야하는 휴대의 불편함이 있습니다<C>

4
또한, 청각장애인의 수화는 양손을 자유롭게 사용해야 하는데 휴대폰 거치에 불편함이 있습니다.<C>

5
이러한 모든 문제를 해결하기 위해 손톡 팀은 간편하게 휴대폰을 접으며 거치할 수 있는 z-flip의 특성을 사용한 실시간 수화 통역 서비스를 개발하였습니다.


6
서비스 손톡이 일상에 녹아들게 된다면 펼쳐질 세상에 대한 재연을 해 보았습니다.

이러한 편리하고도 멋진 아이디어가 실제로 가능한가에 대해 많은 의문이 생길거라고 생각합니다.
손톡 팀도 초반에는 구현이 가능할지에 대한 확신을 갖지 못하고 시작한 프로젝트이기도 합니다.

7
손톡은 실시간 통역, 대화 저장, 핫 키와 같은 핵심 기능을 가지고 있습니다.<C>

8
실시간 통역 기능은 한 공간에 있는 청각장애인과 비장애인의 원활한 의사소통을 보조합니다.

9
대화 저장 기능은 과거에 나누었던 대화를 저장하여 나중에 조회할 수 있도록 합니다.

10
핫 키 기능은 자주 사용하는 문장이나 단어를 한 번의 클릭으로 전달할 수 있게 합니다.

11
이제 손톡 서비스를 여러분들 앞에서 직접 보여드리도록 하겠습니다.
화면을 전환할 동안 잠시 기다려 주시기 바랍니다.

12 - 시연
안드로이드를 사용하는 유저라면 누구나, 쉽게, 구글 플레이스토어에서 앱을 다운로드 받을 수 있습니다. 
회원가입을 하지 않고도 앱 사용을 할 수 있도록 하여 사용자의 접근성을 높였습니다.
현재 16번째 버전인 1.2 버전이 서비스 되고 있습니다.

손톡의 서비스는 크게 대화 기능과 히스토리 기능, 2가지로 구성됩니다.

여기서 z-플립을 접게 된다면 대화를 위한 준비가 완료됩니다.
현재 화면에서는 왼쪽에는 청각장애인이 오른쪽에는 비장애인에 위치해 있습니다.
대화 장면을 시뮬레이션 하도록 하겠습니다.

빠른 사용 버튼을 눌러 청각장애인은 간편하게 처음 만난 상대에게 대화 방식에 대하여 설명해 줄 수 있습니다.

이제, 간단한 대화를 보도록 하겠습니다.

이제 대화를 마치게되면 제목과 TAG를 사용하여 방금 나눈 대화를 저장할 수도 있습니다.
왼쪽으로 스크롤을 하면 히스토리 탭으로 이동을 하고, 여기서 대화 기록을 확인할 수 있습니다.
제목과, TAG, 시간 정보를 통해 대화 내용 열람이 가능합니다.
시연을 마치고, 기술 설명을 드리겠습니다.

13
이러한 놀라운 동작들이 가능하도록 하기 위해 저희 손톡 팀은 다음과 같은 기술을 사용하였습니다.
첫번째로 음성을 문자로, 그리고 문자를 음성으로 바꾸기 위한 NAVER의 Clova API입니다.
다음은 카메라로부터 전달 받은 화면에서 손과 동작을 인식하기 위한 Mediapipe와 CameraX 라이브러리입니다.
마지막으로는 인식된 손과 동작을 수어로 인식하고 문자로 변환해 주는 모델과 TensorFlow Lite를 사용하였습니다.
이 모든 동작들은 믿기지 않겠지만 휴대폰 내부에서 모두 이루어 집니다.
바로, OnDevice AI 입니다. 애플리케이션 하나에 집약되어 있는 기술들에 대해 하나씩 설명드리도록 하겠습니다.

14
비장애인과 청각장애인의 대화 방식으로 나누어 설명드리겠습니다.
먼저, 비장애인의 대화 방식입니다.
비장애인의 음성은 Clova의 STT 기술인 Clova Speech Recognition, CSR sdk를 사용해 구현하였습니다.  
이러한 기술을 사용해 비장애인의 음성을 청각장애인이 텍스트로 볼 수 있게 제공합니다.<C>

15
다음으로는 청각장애인의 대화 방식입니다.
청각장애인의 동작을 수어로 인식하고, 수어로부터 문자를 뽑아내는 것이 이번 손톡 프로젝트의 가장 핵심적이며 어려운 부분이였습니다.  
우선, <C>스마트폰의 CameraX 라이브러리를 사용해 영상 정보를 가져옵니다.  
이 영상 정보에 mediapipe의 handLandmark와 poseLandmark를 적용하여 손과 몸짓의 관절들의 좌표를 얻어옵니다.<C>

16
이러한 관절 정보를 사용하여 데이터 전처리 과정을 거칩니다.
관절의 위치 정보를 담고 있는 좌표 만큼이나 관절의 각도가 수어에 큰 영향을 미치기 때문에 관절간의 각도를 계산하여 구해줍니다.
이러한 과정을 거쳐 만들어진 정보는 수어-텍스트 변환모델을 담고있는 tensorflowlite에 입력 데이터로 전달됩니다.<C>

17
tensorFlowLite는 각 단어와 수화가 일치할 확률을 반환합니다.  
반환된 확률을 텍스트로 변환하는 손톡의 텍스트 알고리즘에 적용시키면 청각장애인의 수어를 유추해 낼 수 있습니다.<C>

18
이제, 마지막 단계 입니다. 이러한 유추한 단어에 Clova Speech API TTS를 사용해 비장애인이 쉽게 들을 수 있는 음성으로 정보가 전달 됩니다.

19
이러한 일련의 과정을 통해 저희 손톡 팀은 청각장애인의 수어를 음성으로, 비장애인의 음성을 텍스트로 변환하는 멋진 애플리케이션을 개발하였습니다.

20
손톡 팀이 이러한 결과물을 낼때 까지 아래와 같은 두 어려움이 있었습니다.
AI Hub에서 제공하는 AIHub Dataset을 사용하려 하였으나, 6주의 기간 동안 사용하기에는 너무 많은 데이터를 학습시켜야 하는 문제가 있었습니다.
또한, 한국수어사전의 수어를 크롤링하여 데이터 세트로 사용하려 하였으나 너무 낮은 화질에 미디어파이프가 관절을 잡아내지 못하는 문제가 있었습니다.<C>

21
이러한 문제를 해결하기 위해 손톡 팀은 335개의 학습 데이터를 직접 제작하였습니다.<C>

22
이러한 데이터를 이용하여 수어-텍스트 변환 모델을 학습 시켜 사용하였습니다.

23
따라서 손톡을 사용한다면, 청각 장애인과의 소통이 원활해 질것으로 기대하며,
갤럭시 z-flip의 새로운 활용 방법으로 활용성이 확대 될 것으로 기대합니다.<C>

24
6명의 손톡 팀원들이 있었기에 6주의 짧은 기간동안 프로젝트를 잘 마칠 수 있었습니다.
AI의 탁성건, 이재홍, 강성구
프론트엔드의 김용우, 동화영, 임서희 였습니다.<C>

25
감사합니다
손과 대화하세요 손톡이였습니다.
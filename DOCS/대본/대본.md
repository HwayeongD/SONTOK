발표 시간 15분  
영상 시간 1.5분
시연 시간  

시연에 앞써, 더미 데이터 몇 개를 넣어두도록 합니다.  

# 1  
(발표장 한쪽에 빠져서 발표를 시작)  
(상황에 따라 달라질 수 있겠지만, 컨설턴트님이 발표 시작을 언급하시면 무대 중앙으로 바로 나가면되고, 아니라면 아래의 맨트)  
지금부터 A609 팀의 발표를 시작하겠습니다  

# 2  
(무대의 중앙에 서서 핸드폰을 잡음, 타자치는 척을 함)  
(안녕하세요) 3초  
(지금부터 A609팀의 발표를 시작하겠습니다) 6초  
(글자를 통한 발표 어떠신가요?) 4초  
(답답하진 않으신가요?) 3초
(...)  
여러분은 지금 청각 장애인과의 대화를 간접적으로 체험해보았습니다.
한 공간에서 서로를 마주하고 있는 상황에서도 텍스트를 통해 의사소통하게 되면 대화에 있어 많은 시간이 소요되고,
서로의 눈을 바라보지 못하는 등의 많은 제약 사항이 생깁니다.

다시 한번 인사드리겠습니다.  
안녕하십니가. A609팀 **손톡**의 발표를 맡은 동화영 입니다.  

# 3
목차는 다음과 같습니다.

# 4
저희는 청각장애인과 비장애인의 **대화의 벽**을 허무는 것에 초점을 두고 프로젝트를 기획하였습니다.  
청각장애인과 비장애인이 한 공간에서 대화를 할 때, 실시간으로 청각장애인의 **수어**가 비장애인에게 **목소리**로 전달되고, 비장애인의 **목소리**는 청각장애인의 **눈**으로 전달된다는 멋진 아이디어를 현실로 만들어 내고자 하였습니다.  

# 5
저희의 서비스 손톡이 일상에 녹아들게 된다면 아마도 이러한 세상이 펼쳐질 것입니다.
(영상)  

# 6
이러한 편리하고도 멋진 아이디어가 실제로 가능한가에 대해 많은 의문이 생길거라고 생각합니다.  
저희 손톡 팀도 초반에는 구현이 가능할지에 대한 확신을 갖지 못하고 시작한 프로젝트이기도 합니다(웃음)  
이제 손톡 서비스를 여러분들 앞에서 직접 보여드리도록 하겠습니다.  

(시연)  
손톡의 서비스는 크게 **대화** 기능과 **히스토리** 기능, 2가지로 구성됩니다.  
안드로이드를 사용하는 유저라면 누구나 쉽게 구글 플레이스토어에서 앱을 다운로드 받을 수 있습니다.  
회원가입을 하지 않고도 앱 사용을 할 수 있도록 하여 사용자의 접근성을 높였습니다.  
먼저 대화 서비스를 시연하도록 하겠습니다.  

(대화창에 들어감)  
손톡은 핸드폰의 카메라를 사용하여 청각장애인의 **손**과 **동작**에 대한 정보를 인식합니다.  
이러한 정보를 사용해 청각장애인의 수어는 비장애인에게 음성으로 전달됩니다.  
반대로 비장애인의 음성이 청각장애인에게는 문자로 전달됩니다.  

"안녕하세요 시연 발표 준비가 완료 되셨나요?"  
(발표, 준비, 끝내다)

이러한 기능 이외에도 손톡을 처음 접하는 비장애인을 위한 안내맨트(안내맨트 누르기, 안내맨트가 종료될 때 까지 잠시 기다리기),
그리고 자주사용하는 단어에 대한 HOT키(HOT키 누르기)를 제공하여 보다 더 원활한 대화를 할 수 있도록 하였습니다.  

이제 대화를 마치게되면 제목(제목 시연하는 사람이 입력해주기)과 TAG를 사용(TAG 몇 개 추가하기)하여 방금 나눈 대화를 저장할 수도 있습니다.  
저장을 하면 열람도 가능해야겠죠?
왼쪽으로 스크롤을 하면 히스토리 탭으로 이동을 하고, 여기서 대화 기록을 확인할 수 있습니다(대화를 종료하고 히스토리로 넘어가고 대화를 보여줌)

# 7
이러한 동작들이 가능하도록 하기 위해 저희 손톡 팀은 다음과 같은 기술을 사용하였습니다.  
첫번째로 음성을 문자로, 그릐고 문자를 음성으로 바꾸기 위한 NAVER의 Clova API입니다.  
다음은 카메라로부터 전달 받은 화면에서 손과 동작을 인식하기 위한 Mediapipe와 CameraX 라이브러리입니다.  
그리고 마지막으로는 인식된 손과 동작을 수어로 인식하고 문자로 변환해 주는 모델과 TensorFlow Lite를 사용하였습니다.  
이 모든 동작들은 믿기지 않겠지만 휴대폰 내부에서 모두 이루어 집니다.  
바로 OnDevice 형식이죠. 애플리케이션 하나에 집약되어 있는 기술들에 대해 하나씩 설명드리도록 하겠습니다.
 
# 8
비장애인과 청각장애인의 대화 방식으로 나누어 설명드리겠습니다.
먼저, 비장애인의 대화 방식입니다.
비장애인의 음성은 Clova의 STT 기술인 Clova Speech Recognition, CSR sdk를 사용해 구현하였습니다.  
이러한 기술을 사용해 비장애인의 음성을 청각장애인이 텍스트로 볼 수 있게 제공합니다.  

# 9
다음으로는 청각장애인의 대화 방식입니다.
청각장애인의 동작을 수어로 인식하고, 수어로부터 문자를 뽑아내는 것이 이번 손톡 프로젝트의 가장 어려운 부분이였습니다.  
우선, 스마트폰의 CameraX 라이브러리를 사용해 영상 정보를 불러옵니다.  
이 영상 정보에 mediapipe의 handLandmark와 poseLandmark를 적용하여 화면에서의 관절들의 좌표를 얻어옵니다.

# 10
이러한 관절 정보를 사용하여 데이터 전처리 과정을 거칩니다.  
관절의 위치 정보를 담고 있는 좌표 만큼이나 관절의 각도가 수어에 큰 영향을 미치기 때문에 관절간의 각도를 계산하여 구해줍니다.  
이러한 과정을 거쳐 만들어진 정보는 수어-텍스트 변환모델을 담고있는 tensorflowlite에 입력데이터로 전달됩니다.  

# 11
tensorFlowLite는 각 단어와 수화가 일치할 확률을 반환합니다.  
반환된 확률을 텍스트로 변환하는 알고리즘에 적용시키면 청각장애인의 수어를 유추해 낼 수 있습니다.  

# 12
이러한 유추한 단어에 Clova Speech API TTS를 사용해 비장애인이 쉽게 들을 수 있는 음성으로 정보가 전달 됩니다.  

# 13  
방금은 제가 비장애인과 청각장애인의 대화 방식을 나누어 설명드렸습니다.
이 대화 방식의 기술을 한 번에 나타내면 다음과 같은 플로우로 진행이 됩니다.

# 14
이제 저희 손톡 팀의 가장 핵심적인 **수어를 음성으로 변환하는 기술**에 대하여 좀 더 자세히 설명드리겠습니다.  
**미디어파이프**를 사용하면 왼손에서 21개, 오른손에서 21개, 그리고 몸짓에서 33개의 좌표값을 각각 얻을 수 있습니다.  

# 15
손과 몸짓에 있어 좌표 만큼이나 중요한 영향을 끼치는 것을 각도라고 생각했습니다.  
따라서 좌표 간의 벡터 내적 연산을 통해 각도를 구하였습니다.  
이 과정을 거쳐 좌표와 각도 값을 포함한 총 190개의 실수형 데이터를 얻었습니다.  

# 16
이제 이 190개의 실수형 데이터를 수어-텍스트 변환 모델에 입력데이터로 넣습니다,  

# 17  
수어-텍스트 변환 모델은 데이터 시간 스텝 사이의 장기적인 의존성을 훈련으로 추론이 가능한 LSTM을 사용해 만들었습니다.  
수어-텍스트 변환 모델을 사용해 단어별 확률값을 얻을 수 있습니다.  

# 18
수어-텍스트 변환 모델로 추론을 하기 위해 저희 손톡 팀은 수어 영상을 매칭시키는 과정을 진행하였습니다.  

# 19
가장 잘 추론하는 모델을 만들기위해, LAG의 수와 에포크의 크기와 같은 파라미터 튜닝 과정을 거쳤으며, 실험을 통해 LAG가 1일 때, 그리고 150의 에포크을 갖게 된다면 가장 높은 정확도를 갖는 모델을 생성할 수 있음을 알았습니다.  

# 20
단어별 확률값을 사용해 하나의 단어를 결정하는 과정은 다음과 같습니다.  
임계값을 지정하여 이 값보다 크면서도 확률이 가장 높은 단어의 인덱스를 증가시킵니다.
이 과정을 반복하다가 t 번 이상 단어가 등장하게 된다면 그 인덱스가 "감사합니다"와 같은 단어를 나타낸다고 결정하였습니다.  
실험을 통해 임계값이 0.8인 이상인 단어가 5회 이상 등장하였다면 결정하도록 하였습니다.  

# 21
이러한 일련의 과정을 통해 저희 손톡 팀은 청각장애인의 수어를 음성으로, 비장애인의 음성을 텍스트로 변환하는 멋진 애플리케이션을 개발하였습니다.  

# 22
발표는 여기서 마치겠습니다.
질문 있으신 분들은 손 들고 질문해주시면 감사하겠습니다.

# 23
지금까지 A609팀 "손과 대화하세요! 손톡"의 발표를 맡은 동화영 이였습니다. 감사합니다.

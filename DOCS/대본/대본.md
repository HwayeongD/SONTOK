발표 시간 15분  
영상 시간 1.5분
시연 시간  

시연에 앞써, 더미 데이터 몇 개를 넣어두도록 합니다.  

# 1  
(발표장 한쪽에 빠져서 발표를 시작)  
(상황에 따라 달라질 수 있겠지만, 컨설턴트님이 발표 시작을 언급하시면 무대 중앙으로 바로 나가면되고, 아니라면 아래의 맨트)  
지금부터 A609 팀의 발표를 시작하겠습니다  

# 2  
(무대의 중앙에 서서 핸드폰을 잡음, 타자치는 척을 함)  
(안녕하세요) 3초  
(지금부터 A609팀의 발표를 시작하겠습니다) 6초  
(글자를 통한 발표 어떠신가요?) 4초  
(답답하진 않으신가요?) 3초
(...)  
여러분은 지금 청각 장애인과의 대화를 간접적으로 채험해 보셨습니다.  
말을 주고 받는 대화에 있어 많은 시간이 소요되고, 서로의 얼굴을 바라보지 못하고 대화를 하는 것에 있어 많은 제약 사항이 생깁니다.  

다시 한번 인사드리겠습니다.  
A609팀 **손톡**의 발표를 맡은 동화영 입니다.  

# 3
목차는 다음과 같습니다.

# 4
우리는 청각장애인과 비장애인의 **대화의 벽**을 허무는 것에 초점을 두고 프로젝트를 기획하였습니다.  
청각장애인과 비장애인이 한 공간에서 대화를 할 때, 실시간으로 청각장애인의 **수화**가 비장애인에게 **목소리**로 전달되고, 비장애인의 **목소리**가 청각장애인의 **눈**으로 전달된다는 멋진 아이디어를 현실로 만들어 내고자 하였습니다.  

# 5
우리의 서비스 손톡이 일상에 녹아들게 된다면 아마도 이러한 세상이 펼쳐질 것입니다.  
(영상)  

# 6
이러한 편리하고도 멋진 아이디어가 실제로 가능한 것 인지에 대한 많은 의문의 생기실 것이라고 생각합니다.  
우리 손톡 팀도 구현이 가능한지에 확신을 갖지 못하고 시작한 프로젝트 이기도 합니다(웃음)  
손톡 서비스를 여러분들 앞에서 보여드리도록 하겠습니다.  

(시연)  
손톡의 서비스는 크게 **대화** 기능과 **히스토리** 기능으로 구성됩니다.  
안드로이드를 사용하는 사용자는 누구나 쉽게 구글 플레이스토어에서 앱을 다운로드 받을 수 있습니다.  
회원가입을 하지 않고도 앱 사용을 할 수 있도록 하여 사용자의 접근성을 높였습니다.  
대화와 관련된 서비스를 먼저 시연하도록 하겠습니다.  

(대화창에 들어감)  
손톡은 핸드폰의 카메라를 사용하여 청각장애인의 **손**과 **동작**에 대한 정보를 인식합니다.  
이러한 정보를 사용하여 청각장애인의 수화는 비장애인에게 음성으로 전달됩니다.  
반대로 비장애인의 음성이 청각장애인에게 문자로 전달됩니다.  

(안녕하세요 오늘 날씨가 많이 춥죠)  
(발표, 끝내다, ?)

이러한 기능 이외에도 손톡을 처음 접하는 비장애인을 위한 안내맨트(안내맨트 누르기, 안내맨트가 종료될 때 까지 잠시 기다리기), 자주사용하는 단어에 대한 HOT키(HOT키 누르기)를 제공하여 원활한 대화를 할 수 있도록 하였습니다.  

대화를 마치게되면 제목(제목 시연하는 사람이 입력해주기)과 TAG를 사용(TAG 몇 개 추가하기)하여 나눈 대화를 저장할 수 있습니다.  

대화에 대한 기록은 히스토리 탭에서 볼 수 있습니다(대화를 종료하고 히스토리로 넘어가고 대화를 보여줌)

# 7
이러한 동작들이 가능하도록 하기 위해 손톡 팀은 다음과 같은 기술을 사용하였습니다.  
음성을 문자로, 문자를 음성으로 바꾸기 위한 Clova,  
카메라로부터 전달 받은 화면에서 손과 동작을 인식하기 위한 Mediapipe와 cameraX,  
인식된 손과 동작을 수화로 인식하고 문자로 전환해 주는 모델과 TensorFlow Lite를 사용하였습니다.  
이 모든 동작들은 믿기지 않겠지만 휴대폰 내부에서 모두 이루어 집니다.  
이러한 믿기지 않는 기술들에 대한 설명을 하도록 하겠습니다.  
 
# 8
비장애인의 음성은 Clova의 STT 기술인 CSR sdk를 사용해 구현하였습니다.  
이러한 기술을 사용해 비장애인의 음성을 청각장애인이 문자로 볼 수 있게 됩니다.  

# 9
청각장애인의 동작을 수화로 인식하고, 수화로부터 문자를 뽑아내는 것이 이번 손톡 프로젝트의 가장 어려운 부분이였습니다.  
우선, 핸드폰의 cameraX를 사용해 영상 정보를 불러오고,  
영상 정보에 mediapipe의 handLandmark와 poseLandmark를 사용해 화면에서의 관절들의 좌표를 얻습니다.

# 10
이러한 관절 정보를 사용하여 데이터 전처리 과정을 거칩니다.  
관절의 좌표 만큼이나 관절을의 각도가 수화에 큰 영향을 미치기 때문에 내적을 사용해 관절간의 각도를 계산하여 구합니다.  
이러한 과정을 거쳐 만들어진 정보는 190개의 실수형 데이터로 변환되어 손톡팀의 수화-텍스트 변환모델을 담고있는 tensorflowlite로 전달됩니다.  

# 11


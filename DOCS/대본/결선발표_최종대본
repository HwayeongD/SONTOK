1.
안녕하세요. 서울 6반 A609 팀의 발표를 맡은 임서희 입니다.
지금부터
서비스 손톡의 발표를 시작하겠습니다.<C>

2.
어떠신가요? 답답하지는 않으신가요?
여러분은 지금, 청각 장애인과의 대화를, 간접적으로 체험해보았습니다.
같은 공간에 있음에도
텍스트를 통해, 의사소통을 하게 되면,
대화를 하는데 오랜 시간이 걸릴 뿐만 아니라
서로의 눈을 바라보며 소통하지 못하는 등
많은 제약 사항이 생깁니다.<C>

3.
손톡팀은 청각장애인과 비장애인의 대화의<C> 벽을

4. 5.
허무는<C> 것에
초점을 두고 프로젝트를 기획하였습니다.<C>

6.
방금과 같이 청각장애인은,
음성을 사용하는 비장애인들과 실시간으로 소통하기 어렵습니다.

하지만 현재 청각장애인에게 서비스를 제공하기 위한 노력이 없는 것은 아닙니다.<C>

7.
해외에서 서비스가 제공되고 있지만, 
한국어를 지원하지 않습니다.<C>

8. 
장갑을 활용한 서비스도 있지만,
추가로 장비를 챙겨야하는 불편함이 있습니다.<C>

9.
또한, 양손을 자유롭게 사용해야 하는 수어의 특성상
일반 휴대폰을 사용할 경우 거치대가 추가로 필요하다는 단점이 있습니다.<C>

10. 11. 12.
이러한 모든 문제를 해결하기 위해,<C>
간편하게 휴대폰을 접으며 거치할 수 있는      삼성전자 갤럭시 z-flip을 사용한
실시간 수어 통역 서비스를 개발하였습니다.<C>

만약 손톡이 일상에 녹아들게 된다면,<C>
어떤 세상이 펼쳐질지 재연해 보았습니다.<C>

13.
(영상)
영상 잘 보고 오셨나요?
이렇게 편리하고도, 멋진!
손톡 서비스를
소개해 드릴 수 있어 기쁩니다.<C>

14. 15.
손톡은 크게<C> 3가지 핵심 기능을 가지고 있습니다.
첫째, 청각장애인과 비장애인의 원활한 대화를 위한 실시간 통역 기능,
둘째, 청각장애인이 자주 사용하는 응답을 제공하는 빠른 사용 기능,
셋째, 함께 나눈 대화를 저장하고 다시 볼 수 있게 해주는 대화 저장 기능 입니다.<C>

16.
첫번째 핵심기능인, 실시간 통역 입니다.
청각장애인과 비장애인은 서로 다른 의사소통 방식을 가지고 있기에 실시간으로 통역해 주는 서비스가 필요합니다.
(1)
손톡은 청각장애인의 수어를 음성으로, 비장애인의 음성은 텍스트로 변환해줍니다.<C>

17.
(2)
이때, 청각장애인의 수어는 문자를 거쳐 비장애인에게 음성으로 전달 됩니다.<C>

18.
(3)
그리고 비장애인의 음성은 청각장애인에게 텍스트로 전달됩니다.<C>

19.
두번째 핵심기능은, 빠른 사용 입니다.
(1)
가장 먼저, 비장애인을 위한 사용설명서 입니다.
스마트폰을 활용해 의사소통을 하려고 하니//
음성으로 대화를 할 수 있다는 안내멘트를 제공합니다.<C>




20.
(2)
네, 아니오, 감사합니다 와 같이 자주 사용하는 단어를 버튼으로 만들어 클릭 한 번으로 대답할 수 있습니다.<C>

21.
(3)
신조어나 고유명사와 같이 수어로 표현하기 어려운 단어를 텍스트로 입력 받을 수 있습니다.
이 텍스트 역시 음성으로 변환되어 비장애인에게 전달됩니다.<C>

22.
마지막 핵심기능인 대화 저장입니다.
//실시간으로 대화한 내용을 저장하여 보관할 수 있습니다.
(1)
대화한 내용을 제목과 태그를 달아 저장해줍니다.<C>

23.
(2)
이렇게 저장한 대화 목록을 조회할 수 있으며<C>

24.
(3)
각각의 대화 목록을 상세히 조회할 수 있어 어떤 대화를 나누었는지를 추억할 수도 있습니다.<C>

26. (화면 전환)
그럼 이제 손톡 서비스를 여러분들 앞에서 직접 보여드리도록 하겠습니다.
회원가입 없이 앱을 사용 할 수 있도록 하여 사용자의 접근성을 높였습니다.

////////////////////// 준비

손톡의 서비스는 크게 대화 기능과 히스토리 기능, 2가지로 구성됩니다.

대화 시작을 눌러 대화 기능을 시연하도록 하겠습니다.
대화 시작 버튼을 누르고  z-플립을 접으면 대화를 위한 준비가 완료됩니다.

사용 설명서 버튼을 눌러
간편하게 처음 만난 상대에게 대화 방식에 대하여 설명해줄 수 있습니다.

지금 제 옆에는 청각장애인 역을 맡은 시연자가 앉아있습니다.
시연자에게 가까이 다가가서 대화를 시작해 보겠습니다.

<말 다 하고 걸어가기>

손톡은 청각장애인과 비장애인이 소통하는 애플리케이션입니다.
실제 사용하는 환경을 보여드리기 위해 잠시 마이크를 내려놓고 육성으로 시연을 진행하는 점 양해 부탁드립니다.

저의 대화 내용은 뒤에 보시는 화면에서 실시간으로 확인하실 수 있습니다.

대화를 시작해보겠습니다.

<시연>
(대화하기)
시연자: (상대방 말 듣기 버튼 클릭)

발표자: 오늘 끝나고 어디 가시나요?

시연자: 동대문 커피 마셔요 (상대방 말 듣기 버튼 클릭)

발표자: 날씨가 많이 춥지 않나요?

시연자: 커피 따뜻해요 마셔요 (상대방 말 듣기 버튼 클릭)

발표자: 따뜻한 커피 좋죠. 커피만 드시나요?

시연자: 책 읽어요

발표자: 책과 함께한 티타임 좋네요.
</시연>

이렇게 대화를 마치게되면 제목과 TAG를 사용하여 방금 나눈 대화를 저장할 수 있습니다.

왼쪽으로 스크롤을 하면 히스토리 탭으로 이동하게 됩니다
여기서 제목과, TAG, 시간 정보를 통해 대화 내용 열람이 가능합니다.<C>

27. (화면 전환)
그럼 시연을 마치고, 기술 설명을 하도록 하겠습니다.
<말 다 마치고 다시 왼쪽(원래 자리)로 돌아가기>

28.
손톡 팀은 <C>실시간성을 보장하기 위해, On-Device AI를 지향하였습니다.

29.
Device 에서 동작하기 위해서<C> Tensorflow Lite를 사용하여 모델을 경량화 하였습니다.

30.
그리고 <C>MediaPipe 를 사용해 영상에서 관절을 추출하는 기술을 최적화하였습니다.

31.
마지막으로 <C>한국어 음성과 텍스트를 변환하기 위해 Clova API를 사용하였습니다.<C>

32.
그럼 비장애인과 청각장애인의 대화 방식으로 나누어 설명드리겠습니다.
먼저, 비장애인의 대화 방식입니다.
STT 기술을 사용해 <C>
비장애인의 음성을 청각장애인이 텍스트로 볼 수 있게 제공합니다.<C>

33.
다음으로는 청각장애인의 대화 방식입니다.
<C>
청각장애인의 수어를 텍스트를 거쳐 음성으로 변환해 줍니다. 이 과정이 이번 손톡 프로젝트의 가장 핵심적이며 어려운 부분이였습니다.<C>

34.
AI Hub에서 제공하는 AIHub Dataset을 사용하려 하였으나,
2.63 테라의 데이터셋을 학습시키기에
15주라는 긴 학습 시간이 필요하였고
짧은 프로젝트 기간 동안 너무 많은 데이터를 학습시켜야 하는 문제가 있었습니다.<C>

35.
이를 해결하기 위해
손톡 팀은 337개의 학습 데이터를 직접<C> 제작하여<C>

36.
수어-텍스트 변환 모델에 사용하였습니다.<C>

37. 38.
손톡을 사용한다면<C>
청각장애인과의 소통이 원활해져 청각장애인의 사회 진출에 도움을 줄 것으로 기대합니다.<C>

39.
또한, 아직은 시작 단계지만 손톡 서비스를 계기로 장애인 서비스에 대한 관심이 늘어나 서비스가 확대되는데 도움이 되었으면 합니다.<C>

40.
6명의 팀원들이 있었기에
6주의 짧은 기간동안 멋진 아이디어를 서비스화 할 수 있었습니다.
AI의 탁성건, 이재홍, 강성구
프론트엔드의 김용우, 동화영, 임서희 였습니다.<C>



41.
끝으로
삼성전자와 삼성 청년 SW 아카데미의 도움이 있었기에
서비스를 실제 앱으로 구현하고 / 또 여러분들께 소개할 수 있었습니다.
저희가 받은 기회를 이렇게 사회에 환원할 수 있게 도움을 주신 관계자분들께 감사드립니다.<C>

42.
감사합니다.<C>
손과 대화하세요 손톡이였습니다.

43.
QnA 받겠습니다.
